{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 1333, 1: 1879, 3: 865, 4: 1489, 0: 935, 5: 157, 6: 137})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "f = open('ESConv.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "processed_data = []\n",
    "for i in data:\n",
    "    pt = i['problem_type']\n",
    "    for j in i['dialog']:\n",
    "        processed_data.append([pt, j['content'], j['speaker'], j['annotation']])\n",
    "sl = []\n",
    "for i in processed_data:\n",
    "    sl.append(len(i[1].split()))\n",
    "## Safely, we remove length 1,2,3\n",
    "to_remove = []\n",
    "for i in range(len(sl)):\n",
    "    if sl[i] <= 3:\n",
    "        to_remove.append(i)\n",
    "for index in sorted(to_remove, reverse=True):\n",
    "    del processed_data[index]\n",
    "# MERGE Issue with parents\n",
    "for i in range(len(processed_data)):\n",
    "    if processed_data[i][0] == 'conflict with parents':\n",
    "        processed_data[i][0] = 'Issues with Parents'\n",
    "# MERGE Into Family Issue\n",
    "for i in range(len(processed_data)):\n",
    "    if processed_data[i][0] == 'Issues with Parents':\n",
    "        processed_data[i][0] = 'Family Issues'\n",
    "    if processed_data[i][0] == 'Issues with Children':\n",
    "        processed_data[i][0] = 'Family Issues'\n",
    "# For now, we care the all 7 categories.\n",
    "kept = ['problems with friends', 'ongoing depression','breakup with partner', 'academic pressure', 'job crisis', 'Family Issues','Sleep Problems']\n",
    "\n",
    "filtered_processed_data = []\n",
    "for i in processed_data:\n",
    "    if i[0] in kept:\n",
    "        filtered_processed_data.append(list(i))\n",
    "        filtered_processed_data[-1][0] = kept.index(filtered_processed_data[-1][0])\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "random.shuffle(filtered_processed_data)\n",
    "lenth = len(filtered_processed_data)\n",
    "train_list = filtered_processed_data[0:int(lenth*0.8)]\n",
    "test_list = filtered_processed_data[int(lenth*0.8):]\n",
    "\n",
    "from collections import Counter\n",
    "# Count labels in training\n",
    "lt = []\n",
    "for i in test_list:\n",
    "    lt.append(i[0])\n",
    "Counter(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import BatchSampler, Sampler, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, BertModel, AdamW, BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Corpora():\n",
    "    \"\"\"\n",
    "        A class that holds the texts and class labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Modification: l represent train_list or test_list\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        \"\"\"\n",
    "            file_path: on MAGIC, it is either\n",
    "                        data-badassnlp/cola_public/raw/in_domain_train.tsv or\n",
    "                        data-badassnlp/cola_public/raw/in_domain_dev.tsv\n",
    "                    on Google Colab, change them to the corresponding paths.\n",
    "        \"\"\"\n",
    "\n",
    "        self.sentences = [s[1] for s in tqdm(l)]\n",
    "        self.labels = [int(s[0]) for s in tqdm(l)]\n",
    "        \n",
    "        \n",
    "class TextClassificationDataSet(Dataset):\n",
    "    \"\"\"\n",
    "        Define a dataset consisting of pairs of (sequence_of_word_indices, class_label).\n",
    "        class_label is either 0 or 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpora, tokenizer: BertTokenizer):\n",
    "\n",
    "        self.corpora = corpora\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        d =  tokenizer.batch_encode_plus(corpora.sentences)\n",
    "        \n",
    "        padded_encoded_inputs = tokenizer.pad(d)\n",
    "        \n",
    "        self.input_ids = torch.tensor(padded_encoded_inputs.input_ids)\n",
    "        self.attention_mask = torch.tensor(padded_encoded_inputs.attention_mask)\n",
    "        self.labels = torch.tensor(corpora.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.corpora.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            Return the idx-th of the rows the self.input_ids, self.attention_mask, self.labels in this order.\n",
    "            Don't do BERT tokenization here as that will be slow.\n",
    "        \"\"\"\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, BERT_model, hidden_layer_size, num_classes):\n",
    "        \n",
    "        super(BERTClassifier, self).__init__()\n",
    "\n",
    "        # loaded pretrained model\n",
    "        self.bert = BERT_model\n",
    "        \n",
    "        # simple neural network that convert embedding of the first token to a class\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(BERT_hidden_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        The following two arguments are tensors from a mini-batch of the input_ids\n",
    "            and attention_mask returned by the BERT tokenizer.\n",
    "            \n",
    "            input_ids: a tensor of shape [batch_size, max_length]\n",
    "            attention_mask: a tensor of shape [batch_size, max_length]\n",
    "            \n",
    "            return: the logits of the sentences in the batch tensor of shape [batch_size, 1, 2]\n",
    "        \"\"\"\n",
    "        # see https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "        # and https://huggingface.co/docs/transformers/main_classes/output\n",
    "        # for the return value of the forward function of BERT\n",
    "\n",
    "        z = self.bert.forward(input_ids, attention_mask)\n",
    "        return torch.softmax(self.classifier(z[1]).unsqueeze(1), dim=-1) \n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you check the availability of GPU when you set the device ID.\n",
    "device = torch.device(0)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "num_classes = 7\n",
    "classifier_hidden_size = 128\n",
    "\n",
    "## if using MAGIC\n",
    "BERT_PATH = '/data/badassnlp/bert-base-uncased/'\n",
    "## if using Google Colab, you need to load the bert model after it downloads the model files.\n",
    "## \n",
    "\n",
    "BERT_hidden_size = 768\n",
    "\n",
    "N_EPOCHS = 5\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/badassnlp/bert-base-uncased/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training corpora ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27180/27180 [00:00<00:00, 1027898.89it/s]\n",
      "100%|██████████| 27180/27180 [00:00<00:00, 1282237.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training iterator ... \n",
      "creating test corpora ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6795/6795 [00:00<00:00, 1186523.55it/s]\n",
      "100%|██████████| 6795/6795 [00:00<00:00, 1261856.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating test dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating test iterator ... \n"
     ]
    }
   ],
   "source": [
    "## 'uncased' means all words are lowered-cased before tokenization\n",
    "## 'base' means the smaller version of BERT (12 layers, 16 heads)\n",
    "## un-comment one of the following two options.\n",
    "\n",
    "# if using MAGIC, load from local BERT folder\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH, local_files_only=True)\n",
    "BERT_model = BertModel.from_pretrained(BERT_PATH, num_labels = 2, output_attentions = False, output_hidden_states = False\n",
    ").to(device)\n",
    "\n",
    "## if using Colab, load from automatically downloaded files. Downloading can take half a minute\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# BERT_model = BertModel.from_pretrained(\"bert-base-cased\", num_labels = 2, output_attentions = False, output_hidden_states = False).to(device)\n",
    "\n",
    "\n",
    "## if using MAGIC\n",
    "print(\"creating training corpora ... \")\n",
    "training_corpora = Corpora(train_list)\n",
    "print(\"creating training dataset ... \")\n",
    "training_dataset = TextClassificationDataSet(training_corpora, tokenizer)\n",
    "print(\"creating training iterator ... \")\n",
    "training_iterator = DataLoader(training_dataset, sampler = RandomSampler(training_dataset), batch_size=BATCH_SIZE)\n",
    "\n",
    "## if using MAGIC\n",
    "print(\"creating test corpora ... \")\n",
    "dev_corpora = Corpora(test_list)\n",
    "print(\"creating test dataset ... \")\n",
    "dev_dataset = TextClassificationDataSet(dev_corpora, tokenizer)\n",
    "print(\"creating test iterator ... \")\n",
    "dev_iterator = DataLoader(dev_dataset, sampler = SequentialSampler(dev_dataset), batch_size=BATCH_SIZE)\n",
    "\n",
    "classifier = BERTClassifier(BERT_model, classifier_hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BERTClassifier(BERT_model, classifier_hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(\"best_model_7classes_balanced_cpu.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "confusion_matrix = []\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    confusion_matrix.append(torch.zeros(num_classes,num_classes))\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        \n",
    "        ids = batch[0].to(device)\n",
    "        msk = batch[1].to(device)\n",
    "        y = (batch[2]).to(device)\n",
    "        logits = model.forward(ids,msk)\n",
    "        loss = criterion(logits.squeeze(1), y) \n",
    "        \n",
    "\n",
    "        for i in range(len(y)):\n",
    "            row = y[i]\n",
    "            col = torch.argmax((logits.squeeze(1))[i])\n",
    "            confusion_matrix[0][row][col] += 1\n",
    "            if abs(row-col) >= 1:\n",
    "                errors.append((ids[i], row, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "850it [00:24, 34.01it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluate(classifier, dev_iterator, criterion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[394.,  82., 192.,  47.,  68., 130.,  22.],\n",
       "         [305., 356., 338., 143., 290., 338., 109.],\n",
       "         [245., 128., 622.,  51.,  87., 157.,  43.],\n",
       "         [ 75.,  91.,  68., 407., 103.,  87.,  34.],\n",
       "         [184., 172., 143., 137., 637., 149.,  67.],\n",
       "         [ 44.,  15.,  22.,   8.,   6.,  59.,   3.],\n",
       "         [ 17.,  18.,  17.,  10.,  13.,  16.,  46.]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['problems with friends',\n",
       " 'ongoing depression',\n",
       " 'breakup with partner',\n",
       " 'academic pressure',\n",
       " 'job crisis',\n",
       " 'Family Issues',\n",
       " 'Sleep Problems']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0124, 0.5046, 0.0940, 0.1059, 0.0686, 0.2070, 0.0076]]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ongoing depression'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentence = \"It is alright. Thank you\"\n",
    "es = torch.tensor([tokenizer.encode(sentence)])\n",
    "msk = torch.ones(len(es[0])).unsqueeze(0)\n",
    "es = es.to(device)\n",
    "msk = msk.to(device)\n",
    "logits = classifier.forward(es,msk)\n",
    "print(logits)\n",
    "kept[torch.argmax(logits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
