{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31606"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 1286, 4: 1488, 1: 1889, 0: 962, 3: 844})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "f = open('ESConv.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "processed_data = []\n",
    "for i in data:\n",
    "    pt = i['problem_type']\n",
    "    for j in i['dialog']:\n",
    "        processed_data.append((pt, j['content']))\n",
    "sl = []\n",
    "for i in processed_data:\n",
    "    sl.append(len(i[1].split()))\n",
    "## Safely, we remove length 1,2,3\n",
    "to_remove = []\n",
    "for i in range(len(sl)):\n",
    "    if sl[i] <= 3:\n",
    "        to_remove.append(i)\n",
    "\n",
    "for index in sorted(to_remove, reverse=True):\n",
    "    del processed_data[index]\n",
    "# For now, we only care the first 5 categories.\n",
    "kept = ['problems with friends', 'ongoing depression','breakup with partner', 'academic pressure', 'job crisis' ]\n",
    "filtered_processed_data = []\n",
    "for i in processed_data:\n",
    "    if i[0] in kept:\n",
    "        filtered_processed_data.append(list(i))\n",
    "        filtered_processed_data[-1][0] = kept.index(filtered_processed_data[-1][0])\n",
    "import random\n",
    "random.seed(10)\n",
    "random.shuffle(filtered_processed_data)\n",
    "lenth = len(filtered_processed_data)\n",
    "train_list = filtered_processed_data[0:int(lenth*0.8)]\n",
    "test_list = filtered_processed_data[int(lenth*0.8):]\n",
    "from collections import Counter\n",
    "# Count labels in testing\n",
    "lts = []\n",
    "for i in test_list:\n",
    "    lts.append(i[0])\n",
    "Counter(lts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    }
   ],
   "source": [
    "list4 = []\n",
    "for i in filtered_processed_data:\n",
    "    list4.extend(i[1].split(\" \"))\n",
    "print(list4[0])\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "c = Counter(list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 23526),\n",
       " ('to', 20333),\n",
       " ('you', 17098),\n",
       " ('a', 12472),\n",
       " ('and', 11657),\n",
       " ('the', 10956),\n",
       " ('', 10171),\n",
       " ('that', 8320),\n",
       " ('is', 7370),\n",
       " ('have', 6539),\n",
       " ('of', 6537),\n",
       " ('for', 6201),\n",
       " ('it', 5863),\n",
       " ('are', 5612),\n",
       " ('my', 5371),\n",
       " ('your', 5110),\n",
       " ('in', 5007),\n",
       " ('with', 4718),\n",
       " ('be', 4468),\n",
       " ('can', 4267),\n",
       " ('but', 3834),\n",
       " ('not', 3686),\n",
       " ('do', 3615),\n",
       " ('i', 3409),\n",
       " ('am', 3367),\n",
       " ('like', 3260),\n",
       " ('me', 3148),\n",
       " ('about', 3067),\n",
       " ('so', 3014),\n",
       " ('on', 2878),\n",
       " ('just', 2835),\n",
       " ('this', 2793),\n",
       " ('will', 2792),\n",
       " ('feel', 2695),\n",
       " ('think', 2648),\n",
       " ('was', 2225),\n",
       " ('get', 2202),\n",
       " (\"I'm\", 2171),\n",
       " ('been', 2107),\n",
       " ('how', 2107),\n",
       " ('as', 2038),\n",
       " ('or', 2018),\n",
       " ('at', 2009),\n",
       " ('help', 2008),\n",
       " ('really', 1998),\n",
       " ('know', 1984),\n",
       " ('what', 1942),\n",
       " ('\\n', 1937),\n",
       " ('if', 1909),\n",
       " ('good', 1909),\n",
       " ('would', 1902),\n",
       " ('out', 1901),\n",
       " ('It', 1828),\n",
       " ('time', 1757),\n",
       " ('they', 1745),\n",
       " (\"don't\", 1653),\n",
       " ('very', 1635),\n",
       " ('he', 1626),\n",
       " ('all', 1607),\n",
       " ('there', 1601),\n",
       " ('some', 1596),\n",
       " ('we', 1491),\n",
       " ('You', 1462),\n",
       " ('has', 1384),\n",
       " ('from', 1363),\n",
       " ('her', 1337),\n",
       " ('she', 1323),\n",
       " ('more', 1303),\n",
       " ('when', 1288),\n",
       " ('up', 1287),\n",
       " ('could', 1241),\n",
       " ('going', 1220),\n",
       " ('want', 1200),\n",
       " ('try', 1171),\n",
       " ('work', 1151),\n",
       " ('things', 1143),\n",
       " ('because', 1119),\n",
       " ('people', 1118),\n",
       " ('talk', 1109),\n",
       " ('any', 1097),\n",
       " ('need', 1092),\n",
       " ('That', 1081),\n",
       " ('had', 1075),\n",
       " ('doing', 1052),\n",
       " ('hard', 1048),\n",
       " ('make', 1045),\n",
       " (\"it's\", 1015),\n",
       " ('go', 984),\n",
       " ('them', 983),\n",
       " ('feeling', 969),\n",
       " ('one', 961),\n",
       " ('job', 944),\n",
       " ('Do', 940),\n",
       " ('Have', 940),\n",
       " ('much', 937),\n",
       " ('an', 934),\n",
       " ('How', 933),\n",
       " ('find', 927),\n",
       " ('take', 924),\n",
       " ('great', 918),\n",
       " ('should', 904),\n",
       " ('What', 904),\n",
       " ('right', 895),\n",
       " ('friends', 891),\n",
       " ('see', 888),\n",
       " ('lot', 885),\n",
       " ('other', 880),\n",
       " ('also', 851),\n",
       " ('him', 847),\n",
       " ('something', 833),\n",
       " ('that.', 832),\n",
       " ('now', 824),\n",
       " ('Thank', 814),\n",
       " ('understand', 813),\n",
       " ('hope', 796),\n",
       " ('better', 796),\n",
       " ('even', 782),\n",
       " ('able', 777),\n",
       " ('you.', 767),\n",
       " ('sure', 760),\n",
       " ('no', 749),\n",
       " ('hear', 741),\n",
       " ('might', 731),\n",
       " ('being', 729),\n",
       " ('through', 723),\n",
       " ('way', 715),\n",
       " ('did', 712),\n",
       " ('who', 709),\n",
       " ('sounds', 707),\n",
       " ('new', 683),\n",
       " (\"It's\", 673),\n",
       " ('someone', 672),\n",
       " ('But', 663),\n",
       " ('sorry', 646),\n",
       " ('best', 631),\n",
       " ('Yes,', 629),\n",
       " ('anything', 627),\n",
       " ('it.', 624),\n",
       " (\"That's\", 615),\n",
       " ('too', 609),\n",
       " ('Is', 608),\n",
       " ('family', 606),\n",
       " (\"I've\", 597),\n",
       " ('keep', 594),\n",
       " ('may', 579),\n",
       " ('by', 578),\n",
       " ('My', 577),\n",
       " ('then', 566),\n",
       " ('having', 565),\n",
       " ('always', 559),\n",
       " (\"that's\", 557),\n",
       " ('over', 556),\n",
       " ('long', 551),\n",
       " ('tell', 550),\n",
       " ('So', 549),\n",
       " ('friend', 549),\n",
       " ('well', 541),\n",
       " ('only', 540),\n",
       " (\"can't\", 540),\n",
       " ('were', 536),\n",
       " ('tried', 532),\n",
       " ('maybe', 532),\n",
       " ('back', 532),\n",
       " ('trying', 529),\n",
       " ('still', 523),\n",
       " ('than', 520),\n",
       " ('getting', 518),\n",
       " ('And', 518),\n",
       " ('into', 515),\n",
       " ('me.', 512),\n",
       " ('talking', 505),\n",
       " ('give', 497),\n",
       " ('its', 491),\n",
       " ('Yes', 489),\n",
       " ('life', 488),\n",
       " ('same', 488),\n",
       " (\"you're\", 487),\n",
       " ('never', 487),\n",
       " ('day', 483),\n",
       " ('myself', 478),\n",
       " ('his', 474),\n",
       " ('thank', 471),\n",
       " ('The', 468),\n",
       " ('If', 467),\n",
       " ('situation', 467),\n",
       " ('our', 461),\n",
       " ('little', 459),\n",
       " ('Maybe', 457),\n",
       " ('yourself', 452),\n",
       " ('today?', 450),\n",
       " ('where', 448),\n",
       " ('their', 440),\n",
       " ('you?', 440),\n",
       " ('after', 439),\n",
       " ('thing', 437),\n",
       " ('got', 433),\n",
       " ('many', 422),\n",
       " ('look', 417),\n",
       " ('love', 413),\n",
       " ('We', 413),\n",
       " ('off', 407),\n",
       " ('does', 406),\n",
       " ('He', 406),\n",
       " ('support', 406),\n",
       " ('why', 402),\n",
       " ('yes', 397),\n",
       " ('bad', 394),\n",
       " ('thought', 394),\n",
       " ('makes', 393),\n",
       " ('say', 392),\n",
       " ('start', 392),\n",
       " ('year', 386),\n",
       " ('Thanks', 384),\n",
       " ('else', 383),\n",
       " ('let', 380),\n",
       " ('down', 377),\n",
       " ('definitely', 377),\n",
       " ('few', 370),\n",
       " ('mind', 368),\n",
       " ('time.', 367),\n",
       " ('too.', 367),\n",
       " ('They', 367),\n",
       " ('kind', 365),\n",
       " ('seems', 365),\n",
       " ('stay', 361),\n",
       " ('now.', 359),\n",
       " ('these', 359),\n",
       " ('before', 358),\n",
       " ('glad', 357),\n",
       " ('care', 357),\n",
       " ('those', 354),\n",
       " ('since', 350),\n",
       " ('difficult', 350),\n",
       " ('looking', 347),\n",
       " ('online', 340),\n",
       " ('She', 339),\n",
       " ('person', 339),\n",
       " ('home', 338),\n",
       " ('working', 338),\n",
       " ('end', 336),\n",
       " ('happy', 334),\n",
       " ('Are', 334),\n",
       " ('another', 330),\n",
       " ('years', 330),\n",
       " ('made', 326),\n",
       " (\"doesn't\", 324),\n",
       " ('which', 323),\n",
       " ('making', 323),\n",
       " ('nice', 322),\n",
       " ('come', 321),\n",
       " ('-', 315),\n",
       " ('most', 313),\n",
       " ('though', 311),\n",
       " (\"didn't\", 309),\n",
       " ('There', 309),\n",
       " ('well.', 308),\n",
       " ('bit', 306),\n",
       " ('everything', 305),\n",
       " ('pandemic', 301),\n",
       " ('live', 299),\n",
       " ('school', 299),\n",
       " ('you,', 297),\n",
       " ('Just', 297),\n",
       " ('Oh', 296),\n",
       " (':)', 296),\n",
       " ('here', 295),\n",
       " ('first', 294),\n",
       " ('actually', 294),\n",
       " ('seem', 294),\n",
       " ('believe', 290),\n",
       " ('us', 289),\n",
       " ('help.', 289),\n",
       " ('sometimes', 288),\n",
       " ('taking', 283),\n",
       " ('wish', 280),\n",
       " ('said', 277),\n",
       " ('feelings', 277),\n",
       " ('No', 277),\n",
       " ('pretty', 276),\n",
       " ('around', 275),\n",
       " ('put', 275),\n",
       " ('problem', 270),\n",
       " ('last', 269),\n",
       " ('This', 268),\n",
       " ('close', 265),\n",
       " ('dont', 264),\n",
       " ('found', 264),\n",
       " ('ask', 263),\n",
       " ('helps', 261),\n",
       " ('move', 261),\n",
       " (',', 260),\n",
       " ('appreciate', 260),\n",
       " ('sad', 259),\n",
       " ('done', 259),\n",
       " ('guess', 258),\n",
       " ('used', 258),\n",
       " ('relationship', 258),\n",
       " ('helped', 257),\n",
       " ('anyone', 256),\n",
       " ('ever', 255),\n",
       " ('own', 255),\n",
       " ('thanks', 253),\n",
       " ('everyone', 252),\n",
       " (\"I'll\", 251),\n",
       " ('during', 250),\n",
       " ('while', 247),\n",
       " ('that,', 244),\n",
       " ('away', 243),\n",
       " ('due', 243),\n",
       " ('must', 242),\n",
       " ('Not', 237),\n",
       " ('Sometimes', 237),\n",
       " ('next', 235),\n",
       " ('thinking', 235),\n",
       " ('both', 234),\n",
       " ('two', 233),\n",
       " ('least', 228),\n",
       " ('different', 228),\n",
       " ('anxiety', 227),\n",
       " ('Well', 227),\n",
       " ('worried', 227),\n",
       " ('use', 226),\n",
       " ('lost', 226),\n",
       " ('tough', 225),\n",
       " (\"haven't\", 225),\n",
       " ('money', 224),\n",
       " ('times', 224),\n",
       " ('idea', 223),\n",
       " ('important', 220),\n",
       " ('.', 218),\n",
       " ('break', 218),\n",
       " ('Well,', 217),\n",
       " ('Yeah', 216),\n",
       " ('without', 215),\n",
       " ('enjoy', 215),\n",
       " ('stress', 215),\n",
       " ('positive', 214),\n",
       " ('Can', 214),\n",
       " ('such', 214),\n",
       " ('together', 213),\n",
       " ('enough', 213),\n",
       " ('im', 211),\n",
       " ('Hello,', 210),\n",
       " ('every', 209),\n",
       " ('especially', 209),\n",
       " ('once', 209),\n",
       " ('better.', 209),\n",
       " ('each', 207),\n",
       " ('idea.', 205),\n",
       " ('started', 204),\n",
       " ('felt', 204),\n",
       " ('pay', 201),\n",
       " ('Yeah,', 201),\n",
       " ('situation.', 201),\n",
       " ('past', 199),\n",
       " ('others', 199),\n",
       " ('nothing', 198),\n",
       " ('this.', 197),\n",
       " ('do.', 195),\n",
       " ('helpful', 195),\n",
       " ('them.', 194),\n",
       " ('deal', 193),\n",
       " ('probably', 192),\n",
       " ('mean', 191),\n",
       " ('already', 189),\n",
       " ('it,', 187),\n",
       " ('depression', 187),\n",
       " ('part', 186),\n",
       " ('it?', 186),\n",
       " ('told', 186),\n",
       " ('When', 183),\n",
       " ('advice', 183),\n",
       " ('call', 182),\n",
       " ('kids', 182),\n",
       " ('place', 182),\n",
       " ('again', 181),\n",
       " ('suggest', 181),\n",
       " ('change', 180),\n",
       " ('worry', 180),\n",
       " ('alone', 180),\n",
       " ('me,', 179),\n",
       " ('job.', 178),\n",
       " ('her.', 177),\n",
       " ('you!', 176),\n",
       " ('out.', 176),\n",
       " ('anxious', 173),\n",
       " ('stressed', 172),\n",
       " ('parents', 171),\n",
       " ('jobs', 170),\n",
       " ('focus', 170),\n",
       " ('similar', 169),\n",
       " ('Hi,', 169),\n",
       " (\"isn't\", 168),\n",
       " ('feels', 167),\n",
       " (\"won't\", 166),\n",
       " ('completely', 165),\n",
       " ('life.', 164),\n",
       " ('until', 164),\n",
       " ('experience', 164),\n",
       " ('day.', 164),\n",
       " ('sound', 163),\n",
       " ('big', 162),\n",
       " ('there.', 162),\n",
       " ('days', 162),\n",
       " ('talked', 162),\n",
       " (\"i'm\", 162),\n",
       " ('husband', 160),\n",
       " ('open', 160),\n",
       " ('trust', 160),\n",
       " ('went', 159),\n",
       " (\"You're\", 158),\n",
       " ('finding', 158),\n",
       " ('work.', 158),\n",
       " ('way.', 158),\n",
       " ('months', 158),\n",
       " ('meet', 157),\n",
       " ('Hi', 157),\n",
       " ('yes,', 156),\n",
       " ('A', 156),\n",
       " ('remember', 156),\n",
       " ('good.', 156),\n",
       " ('leave', 156),\n",
       " ('No,', 154),\n",
       " ('today?\\n', 154),\n",
       " ('social', 154),\n",
       " ('wanted', 153),\n",
       " ('there,', 153),\n",
       " ('understand.', 153),\n",
       " ('you?\\n', 153),\n",
       " ('spend', 153),\n",
       " ('yeah', 152),\n",
       " ('share', 152),\n",
       " ('self', 152),\n",
       " ('thats', 152),\n",
       " ('quite', 151),\n",
       " ('chat', 151),\n",
       " ('issues', 151),\n",
       " ('well,', 149),\n",
       " ('Good', 149),\n",
       " ('okay', 149),\n",
       " ('left', 148),\n",
       " ('3', 147),\n",
       " ('problems', 147),\n",
       " ('him.', 147),\n",
       " ('reach', 146),\n",
       " ('now,', 146),\n",
       " ('time,', 145),\n",
       " ('depressed', 145),\n",
       " ('Yes.', 144),\n",
       " ('ok', 143),\n",
       " ('plan', 142),\n",
       " ('lose', 141),\n",
       " ('Would', 141),\n",
       " ('less', 141),\n",
       " ('boyfriend', 140),\n",
       " ('Im', 140),\n",
       " ('saying', 139),\n",
       " ('today', 138),\n",
       " ('company', 137),\n",
       " ('upset', 137),\n",
       " ('partner', 137),\n",
       " ('week', 136),\n",
       " ('Or', 136),\n",
       " ('group', 136),\n",
       " ('welcome.', 136),\n",
       " ('broke', 136),\n",
       " ('people.', 135),\n",
       " ('coming', 135),\n",
       " ('right.', 134),\n",
       " ('issue', 134),\n",
       " ('Hello', 134),\n",
       " ('ways', 134),\n",
       " ('whole', 134),\n",
       " ('worth', 133),\n",
       " ('house', 133),\n",
       " ('soon', 132),\n",
       " ('learn', 132),\n",
       " ('true.', 131),\n",
       " ('hurt', 131),\n",
       " ('free', 130),\n",
       " ('old', 130),\n",
       " ('covid', 130),\n",
       " ('normal', 129),\n",
       " ('totally', 128),\n",
       " ('listen', 128),\n",
       " ('yourself.', 128),\n",
       " ('happen', 128),\n",
       " ('helping', 128),\n",
       " ('easy', 128),\n",
       " ('rest', 127),\n",
       " ('conversation', 126),\n",
       " ('cannot', 126),\n",
       " ('outside', 125),\n",
       " ('small', 125),\n",
       " ('2', 125),\n",
       " ('took', 125),\n",
       " ('me\\n', 124),\n",
       " ('almost', 122),\n",
       " ('reason', 122),\n",
       " ('quit', 121),\n",
       " ('In', 121),\n",
       " ('struggling', 121),\n",
       " ('usually', 121),\n",
       " ('real', 121),\n",
       " ('safe', 121),\n",
       " ('future', 120),\n",
       " ('again.', 120),\n",
       " ('myself.', 120),\n",
       " ('study', 119),\n",
       " ('though.', 119),\n",
       " ('happened', 119),\n",
       " ('stop', 119),\n",
       " ('please', 119),\n",
       " ('bring', 118),\n",
       " (\"wouldn't\", 118),\n",
       " ('set', 118),\n",
       " (\"you've\", 117),\n",
       " ('Why', 117),\n",
       " ('chatting', 117),\n",
       " ('miss', 117),\n",
       " ('Did', 117),\n",
       " ('dog', 117),\n",
       " ('health', 117),\n",
       " ('recently', 116),\n",
       " ('wonderful', 116),\n",
       " ('sort', 115),\n",
       " ('Perhaps', 115),\n",
       " ('heard', 115),\n",
       " ('college', 113),\n",
       " ('friends.', 113),\n",
       " ('boss', 113),\n",
       " ('ago', 113),\n",
       " ('Christmas', 113),\n",
       " ('up.', 113),\n",
       " ('walk', 112),\n",
       " ('to.', 112),\n",
       " ('often', 112),\n",
       " ('advice.', 112),\n",
       " ('wants', 112),\n",
       " ('fear', 112),\n",
       " ('on.', 112),\n",
       " ('hard.', 112),\n",
       " ('perhaps', 112),\n",
       " ('much.', 111),\n",
       " ('needs', 111),\n",
       " ('seeing', 111),\n",
       " ('today.', 110),\n",
       " ('living', 110),\n",
       " ('sleep', 110),\n",
       " ('luck', 109),\n",
       " ('considered', 109),\n",
       " ('point', 109),\n",
       " ('worked', 109),\n",
       " ('mental', 108),\n",
       " ('For', 108),\n",
       " ('control', 108),\n",
       " ('COVID', 108),\n",
       " ('person.', 108),\n",
       " (\"I'd\", 107),\n",
       " ('couple', 107),\n",
       " ('speak', 107),\n",
       " ('fun', 106),\n",
       " ('year.', 105),\n",
       " ('Please', 105),\n",
       " ('Does', 105),\n",
       " ('Its', 105),\n",
       " (\"aren't\", 105),\n",
       " ('stressful', 105),\n",
       " ('afraid', 104),\n",
       " ('world', 104),\n",
       " ('starting', 104),\n",
       " ('things.', 104),\n",
       " ('about?', 104),\n",
       " ('local', 104),\n",
       " ('busy', 103),\n",
       " (\"wasn't\", 103),\n",
       " ('listening', 103),\n",
       " ('mom', 103),\n",
       " ('asking', 102),\n",
       " ('gets', 102),\n",
       " ('current', 102),\n",
       " ('with.', 102),\n",
       " ('too,', 101),\n",
       " ('class', 101),\n",
       " ('girlfriend', 101),\n",
       " ('true', 101),\n",
       " ('classes', 100),\n",
       " ('suggestions', 100),\n",
       " ('far', 100),\n",
       " ('become', 100),\n",
       " ('cant', 99),\n",
       " ('forward', 99),\n",
       " ('high', 99),\n",
       " ('giving', 99),\n",
       " ('check', 99),\n",
       " ('takes', 98),\n",
       " ('stuff', 98),\n",
       " ('see.', 98),\n",
       " ('too!', 97),\n",
       " ('scared', 97),\n",
       " ('goes', 97),\n",
       " ('Your', 97),\n",
       " ('needed', 97),\n",
       " ('phone', 97),\n",
       " ('wife', 97),\n",
       " ('angry', 96),\n",
       " ('losing', 96),\n",
       " ('face', 96),\n",
       " ('agree', 96),\n",
       " ('course', 96),\n",
       " ('hate', 96),\n",
       " ('it\\n', 95),\n",
       " ('given', 95),\n",
       " ('Covid', 95),\n",
       " ('continue', 95),\n",
       " ('oh', 95),\n",
       " (\"what's\", 95),\n",
       " ('either', 95),\n",
       " ('contact', 95),\n",
       " ('lol', 95),\n",
       " ('step', 95),\n",
       " ('with?', 95),\n",
       " ('head', 94),\n",
       " ('comes', 94),\n",
       " ('cause', 94),\n",
       " ('play', 94),\n",
       " ('night', 94),\n",
       " ('read', 93),\n",
       " ('times.', 93),\n",
       " ('As', 93),\n",
       " ('worse', 93),\n",
       " ('learning', 93),\n",
       " ('possible', 93),\n",
       " ('side', 93),\n",
       " ('all.', 93),\n",
       " ('helpful.', 92),\n",
       " ('it.\\n', 92),\n",
       " ('me.\\n', 92),\n",
       " ('hours', 92),\n",
       " ('moved', 92),\n",
       " (\"Don't\", 92),\n",
       " ('type', 91),\n",
       " ('heart', 91),\n",
       " ('video', 90),\n",
       " ('provide', 90),\n",
       " ('children', 90),\n",
       " ('ideas', 90),\n",
       " ('thoughts', 89),\n",
       " ('deep', 89),\n",
       " ('strong', 89),\n",
       " ('wrong', 89),\n",
       " ('?', 89),\n",
       " ('Of', 88),\n",
       " ('personal', 88),\n",
       " ('groups', 88),\n",
       " ('that!', 87),\n",
       " ('consider', 87),\n",
       " ('that?', 87),\n",
       " ('do,', 87),\n",
       " ('vaccine', 87),\n",
       " ('hopefully', 87),\n",
       " ('staying', 87),\n",
       " ('ex', 87),\n",
       " (\"he's\", 86),\n",
       " ('therapist', 86),\n",
       " ('willing', 86),\n",
       " ('Try', 86),\n",
       " ('position', 86),\n",
       " ('month', 86),\n",
       " ('show', 86),\n",
       " ('recommend', 86),\n",
       " ('5', 86),\n",
       " ('Now', 85),\n",
       " ('full', 85),\n",
       " (\"you'd\", 85),\n",
       " ('this?', 85),\n",
       " ('At', 85),\n",
       " ('family.', 85),\n",
       " (\"she's\", 85),\n",
       " ('dealing', 85),\n",
       " ('works', 84),\n",
       " ('sure.', 84),\n",
       " ('extra', 84),\n",
       " ('is.', 84),\n",
       " ('job,', 84),\n",
       " ('putting', 84),\n",
       " ('financial', 84),\n",
       " ('pandemic.', 84),\n",
       " ('comfortable', 84),\n",
       " ('years.', 83),\n",
       " ('ready', 83),\n",
       " ('easier', 83),\n",
       " ('reaching', 83),\n",
       " ('bothering', 83),\n",
       " ('gone', 83),\n",
       " ('problem.', 83),\n",
       " ('loved', 83),\n",
       " ('asked', 82),\n",
       " ('gave', 82),\n",
       " ('fine', 82),\n",
       " ('write', 82),\n",
       " ('telling', 82),\n",
       " ('cut', 82),\n",
       " ('wait', 82),\n",
       " ('knowing', 82),\n",
       " ('Hello!', 81),\n",
       " ('alone.', 81),\n",
       " ('home.', 81),\n",
       " ('on?', 81),\n",
       " ('taken', 81),\n",
       " ('watch', 81),\n",
       " ('handle', 81),\n",
       " ('daughter', 81),\n",
       " ('seen', 81),\n",
       " ('good,', 80),\n",
       " ('know.', 80),\n",
       " ('hobbies', 80),\n",
       " ('Take', 80),\n",
       " ('came', 80),\n",
       " ('now?', 80),\n",
       " ('4', 80),\n",
       " ('food', 80),\n",
       " ('one.', 79),\n",
       " ('day!', 79),\n",
       " ('matter', 79),\n",
       " ('deserve', 78),\n",
       " ('rough', 78),\n",
       " ('extremely', 78),\n",
       " ('certainly', 78),\n",
       " ('God', 78),\n",
       " ('nervous', 78),\n",
       " ('currently', 78),\n",
       " ('sense', 78),\n",
       " ('hit', 77),\n",
       " ('imagine', 77),\n",
       " ('therapy', 77),\n",
       " ('trouble', 77),\n",
       " ('professional', 76),\n",
       " (\"there's\", 76),\n",
       " ('job?', 76),\n",
       " ('honest', 76),\n",
       " ('help?', 76),\n",
       " ('rather', 76),\n",
       " ('offer', 76),\n",
       " ('weeks', 76),\n",
       " ('clear', 76),\n",
       " ('Hello.', 76),\n",
       " ('understand,', 76),\n",
       " ('Then', 76),\n",
       " ('exactly', 76),\n",
       " ('work,', 75),\n",
       " ('sharing', 75),\n",
       " ('u', 75),\n",
       " ('hoping', 75),\n",
       " ('longer', 75),\n",
       " ('One', 75),\n",
       " ('Any', 75),\n",
       " ('manager', 74),\n",
       " ('lot.', 74),\n",
       " ('..', 74),\n",
       " ('second', 74),\n",
       " ('However,', 73),\n",
       " ('great.', 73),\n",
       " ('moving', 73),\n",
       " ('welcome!', 73),\n",
       " ('Some', 73),\n",
       " ('this,', 73),\n",
       " ('activities', 73),\n",
       " ('called', 73),\n",
       " ('causing', 72),\n",
       " ('friendship', 72),\n",
       " ('cheated', 72),\n",
       " ('feeling.', 72),\n",
       " ('pressure', 72),\n",
       " ('Like', 72),\n",
       " ('moment', 71),\n",
       " ('down.', 71),\n",
       " ('emotions', 71),\n",
       " ('hang', 71),\n",
       " ('sad.', 71),\n",
       " ('All', 71),\n",
       " ('forget', 71),\n",
       " ('suppose', 71),\n",
       " ('list', 70),\n",
       " ('calm', 70),\n",
       " ('state', 70),\n",
       " ('so.', 70),\n",
       " ('chance', 70),\n",
       " ('right?', 70),\n",
       " ('order', 70),\n",
       " ('career', 69),\n",
       " ('you\\n', 69),\n",
       " ('child', 69),\n",
       " ('okay.', 69),\n",
       " ('music', 69),\n",
       " ('here.', 69),\n",
       " ('pass', 69),\n",
       " ('low', 69),\n",
       " ('welcome', 69),\n",
       " ('help,', 69),\n",
       " ('do?', 69),\n",
       " ('daily', 68),\n",
       " ('Sounds', 68),\n",
       " ('resume', 68),\n",
       " ('specific', 68),\n",
       " ('son', 68),\n",
       " ('healthy', 68),\n",
       " ('figure', 68),\n",
       " ('truly', 68),\n",
       " ('Hope', 68),\n",
       " ('Even', 68),\n",
       " ('ones', 68),\n",
       " ('room', 68),\n",
       " ('thing.', 68),\n",
       " ('skills', 68),\n",
       " ('reading', 68),\n",
       " ('holidays', 68),\n",
       " ('support.', 67),\n",
       " ('overcome', 67),\n",
       " ('Because', 67),\n",
       " ('says', 67),\n",
       " ('feel.', 66),\n",
       " ('friend,', 66),\n",
       " ('business', 66),\n",
       " ('together.', 66),\n",
       " ('soon.', 66),\n",
       " ('I’m', 66),\n",
       " ('Ok', 66),\n",
       " ('knew', 66),\n",
       " ('married', 66),\n",
       " ('point.', 66),\n",
       " ('not.', 65),\n",
       " ('it!', 65),\n",
       " ('pain', 65),\n",
       " ('10', 65),\n",
       " ('studying', 65),\n",
       " ('friends,', 65),\n",
       " ('her,', 65),\n",
       " ('interested', 65),\n",
       " ('right,', 65),\n",
       " ('run', 65),\n",
       " ('him,', 65),\n",
       " ('dating', 65),\n",
       " (\"you'll\", 65),\n",
       " ('breakup', 65),\n",
       " ('before.', 64),\n",
       " ('know,', 64),\n",
       " ('speaking', 64),\n",
       " ('negative', 64),\n",
       " ('relax', 64),\n",
       " ('finish', 64),\n",
       " ('doctor', 64),\n",
       " (\"couldn't\", 64),\n",
       " ('seek', 64),\n",
       " ('between', 64),\n",
       " ('hard,', 64),\n",
       " ('can.', 63),\n",
       " ('tend', 63),\n",
       " ('keeping', 63),\n",
       " ('discuss', 63),\n",
       " ('yes.', 63),\n",
       " ('words', 63),\n",
       " ('knows', 63),\n",
       " ('holiday', 63),\n",
       " ('way?', 63),\n",
       " ('sorry.', 63),\n",
       " ('relationships', 63),\n",
       " ('Thats', 63),\n",
       " ('friend.', 62),\n",
       " ('sit', 62),\n",
       " ('day,', 62),\n",
       " ('difficult.', 62),\n",
       " ('wonder', 62),\n",
       " ('using', 62),\n",
       " ('out,', 62),\n",
       " ('work?', 62),\n",
       " ('goals', 62),\n",
       " ('great!', 62),\n",
       " ('amount', 62),\n",
       " ('options', 62),\n",
       " ('avoid', 62),\n",
       " ('realize', 62),\n",
       " ('stuck', 61),\n",
       " ('be.', 61),\n",
       " ('yet', 61),\n",
       " ('turn', 61),\n",
       " ('gotten', 61),\n",
       " ('relate', 61),\n",
       " ('though,', 61),\n",
       " ('follow', 61),\n",
       " ('supposed', 61),\n",
       " ('school.', 60),\n",
       " ('that\\n', 60),\n",
       " ('lives', 60),\n",
       " ('ago.', 60),\n",
       " ('ended', 60),\n",
       " ('certain', 60),\n",
       " ('mind.', 60),\n",
       " ('personally', 60),\n",
       " ('meeting', 60),\n",
       " ('situation,', 60),\n",
       " ('looked', 60),\n",
       " ('lonely', 60),\n",
       " ('fight', 60),\n",
       " ('spending', 60),\n",
       " ('worst', 59),\n",
       " (':)\\n', 59),\n",
       " ('mother', 59),\n",
       " ('behind', 59),\n",
       " ('sister', 59),\n",
       " ('connect', 59),\n",
       " ('Could', 59),\n",
       " ('hour', 59),\n",
       " ('suggestion', 59),\n",
       " ('cope', 59),\n",
       " ('them,', 59),\n",
       " ('Oh,', 59),\n",
       " ('lots', 59),\n",
       " ('exercise', 59),\n",
       " ('interest', 59),\n",
       " ('so,', 59),\n",
       " ('guy', 59),\n",
       " ('understanding', 59),\n",
       " ('Has', 58),\n",
       " ('means', 58),\n",
       " ('available', 58),\n",
       " ('more.', 58),\n",
       " ('explain', 58),\n",
       " ('absolutely', 58),\n",
       " ('instead', 58),\n",
       " ('afford', 58),\n",
       " ('help!', 58),\n",
       " ('exam', 58),\n",
       " ('improve', 58),\n",
       " ('crazy', 58),\n",
       " ('week.', 58),\n",
       " ('situation?', 58),\n",
       " ('cheating', 58),\n",
       " ('now\\n', 57),\n",
       " ('case', 57),\n",
       " ('emotional', 57),\n",
       " ('them?', 57),\n",
       " ('short', 57),\n",
       " ('Hi!', 57),\n",
       " ('learned', 57),\n",
       " ('happens', 57),\n",
       " ('search', 57),\n",
       " ('help\\n', 57),\n",
       " ('area', 57),\n",
       " ('income', 57),\n",
       " ('up,', 57),\n",
       " ('Okay', 57),\n",
       " ('Keep', 57),\n",
       " ('manage', 57),\n",
       " ('From', 57),\n",
       " (\"we're\", 57),\n",
       " ('mad', 57),\n",
       " ('in.', 57),\n",
       " ('wont', 57),\n",
       " ('towards', 56),\n",
       " ('eat', 56),\n",
       " ('best.', 56),\n",
       " ('under', 56),\n",
       " ('wanting', 56),\n",
       " ('idea!', 56),\n",
       " ('Ok.', 56),\n",
       " ('anymore.', 56),\n",
       " ('unemployment', 56),\n",
       " ('15', 56),\n",
       " ('virus', 56),\n",
       " ('us.', 56),\n",
       " ('Okay,', 56),\n",
       " ('later', 56),\n",
       " ('pray', 56),\n",
       " ('motivated', 56),\n",
       " ('back.', 55),\n",
       " ('option', 55),\n",
       " ('doing?', 55),\n",
       " ('medication', 55),\n",
       " ('yea', 55),\n",
       " ('Thanks.', 55),\n",
       " ('time?', 55),\n",
       " ('news', 55),\n",
       " ('no,', 55),\n",
       " ('struggle', 54),\n",
       " ('okay,', 54),\n",
       " ('proud', 54),\n",
       " ('letting', 54),\n",
       " ('particular', 54),\n",
       " ('book', 54),\n",
       " ('thinks', 54),\n",
       " ('guys', 54),\n",
       " ('situations', 54),\n",
       " ('pick', 54),\n",
       " ('Also', 54),\n",
       " ('tired', 54),\n",
       " ('fact', 54),\n",
       " ('places', 54),\n",
       " ('that.\\n', 54),\n",
       " ('met', 54),\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.most_common(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'connected?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB: In this version,\n",
    "# WE ONLY embed for frequent words.\n",
    "# Other words are all converterted to \"N0O0N\"\n",
    "selected = c.most_common(20000)\n",
    "selected_index_word = []\n",
    "for i in selected:\n",
    "    selected_index_word.append(i[0])\n",
    "selected_index_word[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are only considering the 20000 most common words\n",
    "## We convert all other words to N0O0N, which indicate unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to N0O0N\n",
    "def convert(word_list):\n",
    "    for i, v in enumerate(word_list):\n",
    "        if v not in selected_index_word:\n",
    "            word_list[i] = \"N0O0N\"\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.sampler import BatchSampler, Sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED=4321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "class Corpora():\n",
    "    \"\"\"\n",
    "    The class holds training and test corpora.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        # word to index (1-based integers) mapping\n",
    "        self.word_index = {\"N0O0N\":0}\n",
    "        self.index_word = [\"N0O0N\"]\n",
    "        # list of reviews tuples, each of which is (sentence_list, rate),\n",
    "        self.training_reviews = []\n",
    "        # (sentence_list, rate) Same format as training_sentences\n",
    "        self.test_reviews = []\n",
    "\n",
    "        self.max_len = 0\n",
    "\n",
    "\n",
    "    # input: a tuple (reviewList, rate)\n",
    "    # todo: insert values into fields\n",
    "    # Return the list representing all index of words in a review.\n",
    "    def insert_fields(self, input):   \n",
    "        # Sentence list\n",
    "        word_indexes = []\n",
    "        for word in input:\n",
    "            if word not in self.word_index.keys():\n",
    "                self.word_index.update({word:len(self.word_index.keys())}) #No add 1 because 0 is already in\n",
    "                self.index_word.append(word)\n",
    "            # find the index of this word, add to return list\n",
    "            word_indexes.append(self.word_index[word])\n",
    "        if len(word_indexes)>self.max_len:\n",
    "            self.max_len = len(word_indexes)\n",
    "        return word_indexes\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Different than P2, here we \n",
    "    def read_corpus(self, is_training):\n",
    "        if is_training is True:\n",
    "            target = train_list\n",
    "        else:\n",
    "            target = test_list\n",
    "        print(\"reading corpus ...\")\n",
    "        for rate, text in tqdm(target):\n",
    "            input = text.split(\" \")\n",
    "            input = convert(input)\n",
    "            tuple = (self.insert_fields(input), rate)\n",
    "            if is_training: \n",
    "                self.training_reviews.append(tuple)\n",
    "            else:\n",
    "                self.test_reviews.append(tuple)\n",
    "                    \n",
    "                \n",
    "# Inherient Dataset, convert list and int to tensors, load to GPU.\n",
    "class ReviewRateDataset(Dataset):\n",
    "   \n",
    "    def __init__(self, review_rate_pairs): # NB: sequence_pairs is corpora.training_reviews, \n",
    "        # list of (sentence_list, rate)\n",
    "        self.review_rate_pairs = review_rate_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.review_rate_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_list, rate = self.review_rate_pairs[idx] \n",
    "        return torch.tensor(sentence_list), torch.tensor(int(rate))\n",
    "\n",
    "# NB! This class will be in DataLoader function as a parameter for batch_sampler\n",
    "class SortedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "        Each sequence in a mini-batch must of the same lengths, while our sentences\n",
    "        are of various lengths.\n",
    "        We can pad the sentences to the same lengths in each mini-batch.\n",
    "        But if a short and long sentences are in the same mini-batch, more paddings\n",
    "        are needed.\n",
    "        We sort the sentences based on their lengths (in descending order)\n",
    "            and then put sentences with similar lengths in a batch to reduce the paddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        \"\"\"\n",
    "            dataset: an torch.utils.data.DataSet object containing all training sequences\n",
    "            batch_size: the number of sequences to put in a mini-batch\n",
    "        \"\"\"\n",
    "\n",
    "        # The sorting and batching go within this function.      \n",
    "        self.batch_size = batch_size \n",
    "        # Sort the dataset (Based on the length of sentence.)\n",
    "        dataset.review_rate_pairs  = sorted(dataset.review_rate_pairs,key=lambda x:len(x[0]), reverse=True)\n",
    "        self.sorted_lengths = len(dataset)\n",
    "        # Batching: Split the dataset into a list of datasets\n",
    "        self.index_batches = []  \n",
    "        # -- NB: Collate function does not work, so I pad it directly.\n",
    "        for i in range(self.__len__()):\n",
    "            self.index_batches.append(padding_collate_func(ReviewRateDataset(dataset.review_rate_pairs[i*batch_size:i*batch_size+batch_size])))\n",
    "        # Now, each mini-batches is a ReviewRateDataset object\n",
    "        # If else format is needed, may change it latter.\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "            return a Python iterator object that iterates the mini-batchs of\n",
    "                training data indices (not individual indices)\n",
    "        \"\"\"\n",
    "        return iter(self.index_batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sorted_lengths // self.batch_size\n",
    "\n",
    "# NB! This function will be in DataLoader function as a parameter for collate_fn\n",
    "def padding_collate_func(batch):\n",
    "    \"\"\"\n",
    "        Transform pairs of input-output sequences in the batch to be of the same length using the function\n",
    "            torch.nn.utils.rnn.pad_sequence.\n",
    "        batch: An iterator and each element is a pair of (input_sequence, output_sequence).\n",
    "        For POS tagging, len(input_sequence) = len(output_sequence). But for different\n",
    "        pairs in batch, their lengths can differ.\n",
    "\n",
    "        Example: a batch of 3 pairs of input/output sequences\n",
    "                [([1,2,3],[1,1,1]), ([1,2,3,4],[2,2,2,2]), ([1,2,3,4,5],[3,3,3,3,3])]\n",
    "                Note: [] encloses tensors (not numpy arra ys)\n",
    "                \n",
    "                \n",
    "                !!!!!NB QUESTION:  it is the inner [] that encloses tensors, right?\n",
    "                Comment: Batch is an element of a Sampler (see test_p1.py, a l[0] is a batch)\n",
    "                \n",
    "                \n",
    "        return: two tensors (one for input sequence batch and another for output sequence batch).\n",
    "                These tensors are padded with zeros so that all sequences in the same batch\n",
    "                are of the same length.\n",
    "        Example: input_sequence_batch = [[1,2,3,0,0], [1,2,3,4,0], [1,2,3,4,5]],\n",
    "                 output_sequence_batch = [[1,1,1,0,0], [2,2,2,2,0], [3,3,3,3,3]]\n",
    "\n",
    "    \"\"\"\n",
    "    ### Your codes go here (5 points) ###\n",
    "    # Hint: read the article linked at the top of this cell.\n",
    "    \n",
    "    # NOTe\n",
    "    # len(batch[0][0]) == len(batch.sequence_pairs[0]) == the target value (the maximum length for each batch)\n",
    "    # I fill it might be easier to pad sequence_pairs with tuple (0,0), but let's see.\n",
    "    \n",
    "    # Doubles the memory takes by this batch, which is not good\n",
    "    new_l = []\n",
    "    rate = []\n",
    "    for i,j in batch.review_rate_pairs:\n",
    "        new_l.append( torch.tensor(i))\n",
    "        rate.append( torch.tensor(int(j)))\n",
    "    padded = pad_sequence(new_l, batch_first=True, padding_value=0)\n",
    "    #print(\"len \", len(padded), len(rate)) #BUG: NEED ZIP HERE\n",
    "    obj = ReviewRateDataset(list(zip(padded,rate)))\n",
    "    # NB: For now the outter [] is neither tensor nor list, it is an obj!\n",
    "    # Change latter if necessary\n",
    "    # Yes, let's change it to two tensors return\n",
    "    ret1 = []\n",
    "    ret2 = []\n",
    "    for i in obj:\n",
    "        ret1.append(i[0])\n",
    "        ret2.append(i[1])\n",
    "    ret1 = torch.stack(ret1)\n",
    "    ret2 = torch.stack(ret2)\n",
    "    return ret1, ret2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Corpora, Get DataLoader Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading corpus ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25876/25876 [00:13<00:00, 1947.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading corpus ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6469/6469 [00:03<00:00, 1831.64it/s]\n",
      "/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences = 25876\n",
      "Number of test sentences = 6469\n",
      "Number of unique input tokens = 20001\n",
      "Maximal sentence length = 166\n",
      "\n",
      "\n",
      " Creating training Dataset, Sampler, and Iterators...\n",
      "\n",
      "\n",
      " Creating test Dataset, Sampler, and Iterators\n",
      "Training first batch max length = 166\n",
      "Training second batch max length = 90\n",
      "Training last batch max length = 4\n",
      "Training second last batch max length = 4\n"
     ]
    }
   ],
   "source": [
    "# NB: DO NOT RE-RUN THIS CELL\n",
    "# NB: DO NOT RE-RUN THIS CELL\n",
    "# NB: DO NOT RE-RUN THIS CELL\n",
    "# NB: DO NOT RE-RUN THIS CELL\n",
    "# NB: DO NOT RE-RUN THIS CELL\n",
    "# NB: DO NOT RE-RUN THIS CELL\n",
    "\n",
    "#\n",
    "#\n",
    "from torch import optim\n",
    "import time\n",
    "import math\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "corpora = Corpora()\n",
    "\n",
    "corpora.read_corpus(True)\n",
    "corpora.read_corpus(False)\n",
    "\n",
    "print(f'Number of training sentences = {len(corpora.training_reviews)}')\n",
    "print(f'Number of test sentences = {len(corpora.test_reviews)}')\n",
    "print(f'Number of unique input tokens = {len(corpora.word_index)}')\n",
    "print(f'Maximal sentence length = {corpora.max_len}')\n",
    "\n",
    "print(\"\\n\\n Creating training Dataset, Sampler, and Iterators...\")\n",
    "training_dataset = ReviewRateDataset(corpora.training_reviews)\n",
    "training_sampler = SortedBatchSampler(training_dataset, batch_size=BATCH_SIZE)\n",
    "training_iterator = DataLoader(training_dataset,\n",
    "                                  collate_fn = padding_collate_func,\n",
    "                                  batch_sampler = training_sampler)\n",
    "print(\"\\n\\n Creating test Dataset, Sampler, and Iterators\")\n",
    "test_dataset = ReviewRateDataset(corpora.test_reviews)\n",
    "test_sampler = SortedBatchSampler(test_dataset, batch_size=BATCH_SIZE)\n",
    "test_iterator = DataLoader(test_dataset,\n",
    "                              collate_fn = padding_collate_func,\n",
    "                              batch_sampler = test_sampler)\n",
    "\n",
    "print(f'Training first batch max length = {len(list(training_sampler)[0][0][0])}')\n",
    "print(f'Training second batch max length = {len(list(training_sampler)[1][0][0])}')\n",
    "print(f'Training last batch max length = {len(list(training_sampler)[-1][0][0])}')\n",
    "print(f'Training second last batch max length = {len(list(training_sampler)[-2][0][0])}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import embedding, nn\n",
    "# There is really nothing to be stored in this object.\n",
    "# -- But wait, how about self.rnn and self.fc?\n",
    "# -- NB: NOW, I assume that the nn keep weights from the inherentance,\n",
    "# -- And these functions as LSTM and FC will use these weight correctly\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        :param input_dim: size of the vocabulary (number of unique tokens)\n",
    "        :param output_dim: number of unique POS tags \n",
    "        :param emb_dim: embedding dimensionality of each token\n",
    "        :param hid_dim: number of hidden neurons of a hidden state/cell\n",
    "        :param n_layers: number of RNN layers (2 for faster training)\n",
    "        :param dropout: dropout rate between 0 and 1at the embedding layer and rnn\n",
    "        :param bidirectional: 1 if use bidirectional and 0 if don't\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(corpora.word_index), 5)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "\n",
    "        :param src: a [batch_size, sentence_len] array.\n",
    "                     Each row is a sequence of word indices and each column represents a position in the sequence.\n",
    "        :return: the predicted logits at each position. \n",
    "        \"\"\"\n",
    "        emb = self.embedding(src)\n",
    "        z = torch.sum(emb,dim=1)\n",
    "        d = torch.softmax(z, dim=-1)\n",
    "\n",
    "        # Notice that we compute d in the process of training\n",
    "        # No need to compute two times of softmax, which will make prediction inaccurate.\n",
    "        return z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(corpora.word_index)\n",
    "\n",
    "# initialize the model\n",
    "featureExtractor = FeatureExtractor(INPUT_DIM).cuda(0)\n",
    "\n",
    "# Glove Embedding here?\n",
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "featureExtractor.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(featureExtractor.parameters())\n",
    "\n",
    "\n",
    "tmp = optimizer.state_dict()\n",
    "tmp[\"param_groups\"][0][\"lr\"] = 0.0005\n",
    "optimizer.load_state_dict(tmp)\n",
    "print(optimizer)\n",
    "\n",
    "# we use 0 to represent padded POS tags and the loss function should ignore that.\n",
    "# we calculate the sum of losses of pairs in each batch\n",
    "PAD_INDEX = 0\n",
    "\n",
    "\n",
    "# input: vector of [length, output_dim], integer (score)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -- The Iterator is a Dataloader object. \n",
    "# -- Use for loop in iterator.batch_sampler to access each batches\n",
    "# -- In this case, each batches is having length 128\n",
    "\n",
    "# -- Need to Figure out: The way to compute loss for RNN\n",
    "num_epochs_train = 0\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "     \n",
    "    global num_epochs_train\n",
    "        \n",
    "    \n",
    "    if num_epochs_train == 0:\n",
    "        tmp = optimizer.state_dict()\n",
    "        tmp[\"param_groups\"][0][\"lr\"] = 0.008\n",
    "        \n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_batchs = 0\n",
    "    total = 0\n",
    "    random.shuffle(iterator.batch_sampler.index_batches)\n",
    "    \n",
    "    # batch[0]: the word batch\n",
    "    # batch[1]: the tag batch (target)\n",
    "    print(\"training ...\")\n",
    "    for i, batch in tqdm(enumerate(iterator.batch_sampler)):\n",
    "        # NB: SURPRISE! THE LINE BELOW BOOST TEST ACCURACY\n",
    "        optimizer.zero_grad()\n",
    "#         #skip first batch\n",
    "#         if i == 0:\n",
    "#             continue\n",
    "        num_batchs += 1\n",
    "        z = featureExtractor.forward(batch[0].cuda(0))\n",
    "        #a = torch.softmax(z,dim=-1)\n",
    "        loss = 0\n",
    "        # softmax of logit\n",
    "        d = torch.softmax(z,dim=-1)\n",
    "        # cross entropy loss of softmax and score\n",
    "        loss=criterion(d,(batch[1]).cuda(0))/BATCH_SIZE\n",
    "        loss.backward()\n",
    "        # Clips gradient norm of an iterable of parameters.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        total += 1\n",
    "    num_epochs_train += 1\n",
    "\n",
    "    return epoch_loss /total\n",
    "\n",
    "confusion_matrix = []\n",
    "num_epochs = 0\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    confusion_matrix.append(torch.zeros(5,5))\n",
    "    global num_epochs\n",
    "    for i, batch in tqdm(enumerate(iterator.batch_sampler)):\n",
    "        z = featureExtractor.forward(batch[0].cuda(0))\n",
    "        loss = 0\n",
    "        # softmax of logit\n",
    "        d = torch.softmax(z,dim=-1)\n",
    "        # cross entropy loss of softmax and score\n",
    "        loss=criterion(d,(batch[1]).cuda(0))/BATCH_SIZE\n",
    "        epoch_loss += loss.item()\n",
    "        total += 1\n",
    "        \n",
    "        # Load in confusion_matrix\n",
    "        for i in range(len(d)):\n",
    "            row = batch[1][i]\n",
    "            col = torch.argmax(d[i])\n",
    "            confusion_matrix[num_epochs][row][col] += 1\n",
    "     \n",
    "    num_epochs += 1\n",
    "        \n",
    "    return epoch_loss/total\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch start:  0\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1049.60it/s]\n",
      "101it [00:00, 222.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 0s\tTrain Loss: 1.243 | Test Loss: 1.456\n",
      "epoch start:  1\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 998.32it/s] \n",
      "101it [00:00, 232.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 0s\tTrain Loss: 1.241 | Test Loss: 1.456\n",
      "epoch start:  2\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1005.70it/s]\n",
      "101it [00:00, 233.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 0s\tTrain Loss: 1.239 | Test Loss: 1.456\n",
      "epoch start:  3\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1005.22it/s]\n",
      "101it [00:00, 233.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 0s\tTrain Loss: 1.237 | Test Loss: 1.456\n",
      "epoch start:  4\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1002.68it/s]\n",
      "101it [00:00, 233.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 0s\tTrain Loss: 1.235 | Test Loss: 1.456\n",
      "epoch start:  5\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1003.21it/s]\n",
      "101it [00:00, 233.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 0s\tTrain Loss: 1.233 | Test Loss: 1.456\n",
      "epoch start:  6\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1007.38it/s]\n",
      "101it [00:00, 230.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0m 0s\tTrain Loss: 1.231 | Test Loss: 1.456\n",
      "epoch start:  7\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 999.23it/s] \n",
      "101it [00:00, 233.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0m 0s\tTrain Loss: 1.229 | Test Loss: 1.455\n",
      "epoch start:  8\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 998.61it/s] \n",
      "101it [00:00, 232.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0m 0s\tTrain Loss: 1.227 | Test Loss: 1.455\n",
      "epoch start:  9\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1093.82it/s]\n",
      "101it [00:00, 302.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0m 0s\tTrain Loss: 1.226 | Test Loss: 1.455\n",
      "epoch start:  10\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1315.13it/s]\n",
      "101it [00:00, 304.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Time: 0m 0s\tTrain Loss: 1.224 | Test Loss: 1.455\n",
      "epoch start:  11\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1329.90it/s]\n",
      "101it [00:00, 303.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Time: 0m 0s\tTrain Loss: 1.222 | Test Loss: 1.455\n",
      "epoch start:  12\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1332.45it/s]\n",
      "101it [00:00, 305.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Time: 0m 0s\tTrain Loss: 1.220 | Test Loss: 1.455\n",
      "epoch start:  13\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1330.83it/s]\n",
      "101it [00:00, 304.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Time: 0m 0s\tTrain Loss: 1.218 | Test Loss: 1.455\n",
      "epoch start:  14\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1334.74it/s]\n",
      "101it [00:00, 305.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Time: 0m 0s\tTrain Loss: 1.217 | Test Loss: 1.455\n",
      "epoch start:  15\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1330.50it/s]\n",
      "101it [00:00, 304.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Time: 0m 0s\tTrain Loss: 1.215 | Test Loss: 1.455\n",
      "epoch start:  16\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1331.94it/s]\n",
      "101it [00:00, 304.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Time: 0m 0s\tTrain Loss: 1.213 | Test Loss: 1.455\n",
      "epoch start:  17\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1337.98it/s]\n",
      "101it [00:00, 304.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Time: 0m 0s\tTrain Loss: 1.212 | Test Loss: 1.455\n",
      "epoch start:  18\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1332.00it/s]\n",
      "101it [00:00, 304.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Time: 0m 0s\tTrain Loss: 1.210 | Test Loss: 1.455\n",
      "epoch start:  19\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1329.12it/s]\n",
      "101it [00:00, 303.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Time: 0m 0s\tTrain Loss: 1.208 | Test Loss: 1.455\n",
      "epoch start:  20\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1339.22it/s]\n",
      "101it [00:00, 304.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Time: 0m 0s\tTrain Loss: 1.207 | Test Loss: 1.455\n",
      "epoch start:  21\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1329.42it/s]\n",
      "101it [00:00, 303.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Time: 0m 0s\tTrain Loss: 1.205 | Test Loss: 1.455\n",
      "epoch start:  22\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1327.31it/s]\n",
      "101it [00:00, 304.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Time: 0m 0s\tTrain Loss: 1.204 | Test Loss: 1.455\n",
      "epoch start:  23\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1336.31it/s]\n",
      "101it [00:00, 277.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Time: 0m 0s\tTrain Loss: 1.202 | Test Loss: 1.455\n",
      "epoch start:  24\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1307.88it/s]\n",
      "101it [00:00, 302.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Time: 0m 0s\tTrain Loss: 1.201 | Test Loss: 1.455\n",
      "epoch start:  25\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1306.57it/s]\n",
      "101it [00:00, 302.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Time: 0m 0s\tTrain Loss: 1.199 | Test Loss: 1.455\n",
      "epoch start:  26\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1311.87it/s]\n",
      "101it [00:00, 300.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Time: 0m 0s\tTrain Loss: 1.198 | Test Loss: 1.455\n",
      "epoch start:  27\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1295.01it/s]\n",
      "101it [00:00, 301.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Time: 0m 0s\tTrain Loss: 1.196 | Test Loss: 1.455\n",
      "epoch start:  28\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1318.27it/s]\n",
      "101it [00:00, 303.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Time: 0m 0s\tTrain Loss: 1.195 | Test Loss: 1.455\n",
      "epoch start:  29\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [00:00, 1310.67it/s]\n",
      "101it [00:00, 302.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Time: 0m 0s\tTrain Loss: 1.194 | Test Loss: 1.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):  \n",
    "    print(\"epoch start: \", epoch)  \n",
    "    start_time = time.time()\n",
    "    training_loss = train(featureExtractor, training_iterator, optimizer, criterion, CLIP)\n",
    "    training_losses.append(training_loss)\n",
    "    test_loss = evaluate(featureExtractor, test_iterator, criterion)\n",
    "    test_losses.append(test_loss)  \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss \n",
    "        torch.save(featureExtractor.state_dict(), '5_emb.pt')\n",
    "        \n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s', end='')\n",
    "    print(f'\\tTrain Loss: {training_loss:.3f} | Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9e2164b4d0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set()\n",
    "x = np.arange(len(training_losses))\n",
    "plt.plot(x, training_losses, label = 'training loss')\n",
    "plt.plot(x, test_losses, label = 'test loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[279., 289., 211.,  29., 153.],\n",
      "        [137., 999., 300.,  92., 360.],\n",
      "        [137., 398., 538.,  53., 159.],\n",
      "        [ 62., 242.,  89., 290., 160.],\n",
      "        [ 90., 451., 180.,  68., 698.]])\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix[89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diag:  tensor(2807.)\n"
     ]
    }
   ],
   "source": [
    "diagnal = 0\n",
    "\n",
    "ep =89\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        distance+= confusion_matrix[ep-1][i][j]*abs(i-j)\n",
    "        if i == j:\n",
    "            diagnal += confusion_matrix[ep-1][i][j]\n",
    "print(\"diag: \",diagnal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing this 2807 to the BERT: 3167\n",
    "\n",
    "# Cons:\n",
    "1. Not as accurate\n",
    "\n",
    "# Pros:\n",
    "1. Light weighted (the size of pt file is 401kb v.s BERT 438mb)\n",
    "2. Fast train\n",
    "3. Does not consider relationship between words, so we can retrieve Keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kept = ['problems with friends', 'ongoing depression','breakup with partner', 'academic pressure', 'job crisis' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureExtractor.load_state_dict(torch.load(\"5_emb.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:  How are you doing today? It is not very good. How does it feels like? I think we can think. We need to talk. \n",
      "sq:  tensor([[   15,    16,    60,   410,   145,    63,   189,   223,   549,  1673,\n",
      "            15,    13,    12,  4100, 10432,     1,    70,   200,    91,  1805,\n",
      "           981,   165,    28,  5042]], device='cuda:0') \n",
      "prediction:  tensor([[0.0459, 0.7510, 0.1518, 0.0026, 0.0488]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "example1 = \"How are you doing today? It is not very good. How does it feels like? I think we can think. We need to talk.\"\n",
    "words1 = example1.split(\" \")\n",
    "seq1 = []\n",
    "for word in words1:\n",
    "    if word in corpora.index_word:\n",
    "        seq1.append(corpora.word_index[word])\n",
    "    else:\n",
    "        seq1.append(0)\n",
    "seq1 = torch.tensor([seq1]).cuda(0)\n",
    "z = featureExtractor.forward(seq1)\n",
    "d = torch.softmax(z,dim=-1)\n",
    "print(\"Example: \", example1, \"\\nsq: \", seq1, \"\\nprediction: \", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = featureExtractor.embedding(torch.tensor(list(range(20000))).cuda(0))\n",
    "NUM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 100 words for Probelm with Friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friendship\n",
      "friend.\n",
      "friendship.\n",
      "friends\n",
      "\n",
      "friend\n",
      "anime\n",
      "friend?\n",
      "friend,\n",
      "friends\n",
      "friendships\n",
      "lie\n",
      "conflict\n",
      "friendship?\n",
      "invite\n",
      "hang\n",
      "friends.\n",
      "building\n",
      "dear.\n",
      "having?\n",
      "\n",
      "friendships.\n",
      "fight\n",
      "\n",
      "behavior\n",
      "condition.\n",
      "back.\n",
      "\n",
      "We've\n",
      "invited\n",
      "empty\n",
      "from.\n",
      "\n",
      ".\n",
      "\n",
      "O\n",
      "them?\n",
      "misunderstanding.\n",
      "Trump\n",
      "friends.\n",
      "\n",
      "judge\n",
      "nephew\n",
      "ignoring\n",
      "neighbors\n",
      "friend\n",
      "\n",
      "shame\n",
      "Great,\n",
      "upset\n",
      "steps\n",
      "raising\n",
      "loose\n",
      "acting\n",
      "Corona\n",
      "group\n",
      "change.\n",
      "borrowed\n",
      "emotional\n",
      "count\n",
      "army\n",
      "interests\n",
      "seriously.\n",
      "!\n",
      "dealt\n",
      "hurt\n",
      "Friday.\n",
      "ya.\n",
      "trust?\n",
      "contracted\n",
      "refuse\n",
      "disgust\n",
      "Canada\n",
      "sweet.\n",
      "awkward.\n",
      "made.\n",
      "(so-called)\n",
      "asking\n",
      "pre\n",
      "parties\n",
      "asking!\n",
      "person?\n",
      "offended\n",
      "divorces\n",
      "bubble\n",
      "personality.\n",
      "tough.\n",
      "\n",
      "message.\n",
      "hmm\n",
      "behaved\n",
      "stated\n",
      "Thanks!\n",
      "\n",
      "she's\n",
      "inappropriate,\n",
      "recall\n",
      "beating\n",
      "extreme\n",
      "advantage\n",
      "topic\n",
      "fine.\n",
      "\n",
      "LGBT\n",
      "typed?\n",
      "\n",
      "Anime\n",
      "clubs\n",
      "before?\n",
      "\n",
      "snowing\n",
      "arrested\n",
      "hours.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problem_with_friends = torch.torch.topk(embeddings[:,0], NUM).indices\n",
    "for i in range(NUM):\n",
    "    print(corpora.index_word[problem_with_friends[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 100 words for Ongoing Depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "death\n",
      "depression.\n",
      "depressed.\n",
      "depression,\n",
      "store\n",
      "die\n",
      "Medium\n",
      "vitamin\n",
      "dad\n",
      "Got\n",
      "dog?\n",
      "start?\n",
      "\n",
      "celebration\n",
      "fiancée\n",
      "family?\n",
      "\n",
      "mask,\n",
      "rut.\n",
      "black\n",
      "died\n",
      "cancer.\n",
      "son's\n",
      "depression?\n",
      "relatively\n",
      "cycle\n",
      "Depression\n",
      "recognize\n",
      "church\n",
      "ambulance\n",
      "coast\n",
      "planner\n",
      "insight\n",
      "demands\n",
      "granddaughter\n",
      "sexual\n",
      "everything?\n",
      "spirits\n",
      "kitchen\n",
      "mindfulness\n",
      "movies.\n",
      "workouts\n",
      "rare\n",
      "sad,\n",
      "mortality\n",
      "trips\n",
      "depression\n",
      "\n",
      "Florida\n",
      "bother\n",
      "dwelling\n",
      "m\n",
      "discuss?\n",
      "much\n",
      "\n",
      "in\n",
      "\n",
      "quarantine.\n",
      "hotel\n",
      "attacks\n",
      "coughing\n",
      "rut\n",
      "battle\n",
      "beach\n",
      "husband\n",
      "sport\n",
      "consider\n",
      "\n",
      "words\n",
      "\n",
      "symptoms.\n",
      "selfish\n",
      "dear\n",
      "See\n",
      "Poor\n",
      "accident\n",
      "teh\n",
      "picturing\n",
      "passing\n",
      "VERY\n",
      "funk\n",
      "n\n",
      "loss?\n",
      "long\n",
      "\n",
      "minimal\n",
      "wisdom\n",
      "chilly\n",
      "teenager\n",
      "fan\n",
      "hotels\n",
      "Recently\n",
      "employment.\n",
      "job..\n",
      "boyfriends\n",
      "place\n",
      "\n",
      "celebrate\n",
      "pets.\n",
      "evictions\n",
      "paranoid\n",
      "signing\n",
      "teeth\n",
      "mother\n",
      "shy.\n",
      "positive,\n",
      "d\n",
      "lake\n",
      "husband's\n"
     ]
    }
   ],
   "source": [
    "ongoing_depression = torch.torch.topk(embeddings[:,1], NUM).indices\n",
    "for i in range(NUM):\n",
    "    print(corpora.index_word[ongoing_depression[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 100 words for Breakup with Partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breakup\n",
      "ex\n",
      "breakup.\n",
      "breakups\n",
      "partner\n",
      "breakup?\n",
      "Doug\n",
      "Breakups\n",
      "dumped\n",
      "split\n",
      "divorce\n",
      "cheat\n",
      "relationship\n",
      "relationship?\n",
      "wasted\n",
      "relationship,\n",
      "broke\n",
      "suitation\n",
      "together?\n",
      "idea?\n",
      "him?\n",
      "breakup,\n",
      "love\n",
      "\n",
      "marriage\n",
      "closure.\n",
      "break\n",
      "cheated\n",
      "unfaithful\n",
      "awkward\n",
      "broken\n",
      "surprise\n",
      "relationship?\n",
      "\n",
      "problem?\n",
      "occurrence\n",
      "sudden.\n",
      "men\n",
      "kindness.\n",
      "relationships\n",
      "Keri.\n",
      "instead.\n",
      "both.\n",
      "married\n",
      "hurt.\n",
      "description\n",
      "problematic\n",
      "blew\n",
      "laughing\n",
      "comic\n",
      "met\n",
      "too\n",
      "\n",
      "\n",
      "Tinder\n",
      "text,\n",
      "engaged\n",
      "but,\n",
      "denies\n",
      "partner.\n",
      "partner?\n",
      "relationship.\n",
      "porn\n",
      "man\n",
      "dump\n",
      "together\n",
      "connected\n",
      "happened\n",
      "\n",
      "fight.\n",
      "LOT\n",
      "want.\n",
      "Here's\n",
      "deserved\n",
      "smart,\n",
      "that!\n",
      "\n",
      "marry\n",
      "gym.\n",
      "x\n",
      "group,\n",
      "marriage.\n",
      "ex.\n",
      "memories\n",
      "emotions\n",
      "there?\n",
      "there's\n",
      "hobby,\n",
      "anniversary\n",
      "u\n",
      "\n",
      "pretending\n",
      "Crossfit\n",
      "intimate\n",
      "reminded\n",
      "woman.\n",
      "relationships.\n",
      "helped.\n",
      "\n",
      "girlfriend\n",
      "breaking\n",
      "damn\n",
      "neglect\n",
      "enjoy?\n",
      "\n",
      "cheat\n",
      "\n",
      "detail\n",
      "Rob,\n",
      "guy.\n"
     ]
    }
   ],
   "source": [
    "breakup_with_partner = torch.torch.topk(embeddings[:,2], NUM).indices\n",
    "for i in range(NUM):\n",
    "    print(corpora.index_word[breakup_with_partner[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 100 words for Academic Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study\n",
      "academic\n",
      "studying\n",
      "semester\n",
      "exam\n",
      "exams\n",
      "grade\n",
      "professor\n",
      "class\n",
      "grades\n",
      "school\n",
      "professors\n",
      "finals\n",
      "test\n",
      "fail\n",
      "school,\n",
      "school?\n",
      "study.\n",
      "advisor\n",
      "parents\n",
      "class.\n",
      "study?\n",
      "classes\n",
      "progress\n",
      "class,\n",
      "campus\n",
      "renewed\n",
      "compare\n",
      "failing\n",
      "math\n",
      "test.\n",
      "me?\n",
      "\n",
      "approach\n",
      "assignment\n",
      "presentation\n",
      "exams.\n",
      "school.\n",
      "\n",
      "pass\n",
      "schedule\n",
      "programs\n",
      "students\n",
      "pressures\n",
      "college\n",
      "exams?\n",
      "reward\n",
      "library\n",
      "stress?\n",
      "major?\n",
      "overdo\n",
      "semester\n",
      "\n",
      "program\n",
      "visa\n",
      "university\n",
      "failure\n",
      "studying.\n",
      "explain\n",
      "grades.\n",
      "classes,\n",
      "teacher\n",
      "loan\n",
      "biking\n",
      "procrastinating\n",
      "tutors\n",
      "studies\n",
      "app\n",
      "debt\n",
      "economics\n",
      "learning\n",
      "activity,\n",
      "required\n",
      "Exams\n",
      "poorly\n",
      "schooling\n",
      "courses,\n",
      "majors\n",
      "deadlines\n",
      ":/\n",
      "stage.\n",
      "graduate.\n",
      "covid.\n",
      "\n",
      "rattle\n",
      "rest\n",
      "\n",
      "hope.\n",
      "\n",
      "amazing,\n",
      "teacher?\n",
      "almost\n",
      "done\n",
      "\n",
      "season.\n",
      "study,\n",
      "smart\n",
      "managing\n",
      "classmates\n",
      "credits\n",
      "say.\n",
      "\n",
      "understanding\n",
      "form\n",
      "experience?\n",
      "\n",
      "telling\n",
      "speaking\n",
      "\n",
      "studies?\n"
     ]
    }
   ],
   "source": [
    "academic_pressure = torch.torch.topk(embeddings[:,3], NUM).indices\n",
    "for i in range(NUM):\n",
    "    print(corpora.index_word[academic_pressure[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 100 words for Job Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boss\n",
      "boss.\n",
      "HR\n",
      "supervisor\n",
      "jobs.\n",
      "job?\n",
      "\n",
      "co-worker\n",
      "unemployment\n",
      "job.\n",
      "job\n",
      "\n",
      "gig\n",
      "job\n",
      "manager\n",
      "laid\n",
      "job?\n",
      "opportunities\n",
      "typing\n",
      "warehouse\n",
      "business\n",
      "colleagues\n",
      "resume\n",
      "income.\n",
      "money\n",
      "\n",
      "reduce\n",
      "company\n",
      "Uber\n",
      "degrees\n",
      "employee\n",
      "positions\n",
      "Well.\n",
      "jobs\n",
      "resume\n",
      "\n",
      "upper\n",
      "angle\n",
      "HR.\n",
      "Currently\n",
      "expenses\n",
      "director\n",
      "friend!\n",
      "jobs?\n",
      "Human\n",
      "uber\n",
      "boat.\n",
      "coworkers\n",
      "co-workers\n",
      "degree\n",
      "\n",
      "update\n",
      "employment\n",
      "ran\n",
      "plan?\n",
      "Job\n",
      "staffs\n",
      "bananas\n",
      "stores\n",
      "hiring.\n",
      "items\n",
      "burned\n",
      "boss,\n",
      "far!\n",
      "anxiety,\n",
      "resume,\n",
      "jobs.\n",
      "\n",
      "approaching\n",
      "have,\n",
      "submitted\n",
      "ya,\n",
      "manager?\n",
      "Me\n",
      "expenses.\n",
      "motivational\n",
      "dunno.\n",
      "work?\n",
      "\n",
      "industry\n",
      "channel.\n",
      "step.\n",
      "walked\n",
      "jobs,\n",
      "encouraged\n",
      "2021!\n",
      "promising\n",
      "salary\n",
      "relax?\n",
      "January\n",
      "offices\n",
      "presents\n",
      "Hey,\n",
      "cool,\n",
      "jobs?\n",
      "\n",
      "releasing\n",
      "principal\n",
      "cold.\n",
      "modern\n",
      "field.\n",
      "remind\n",
      "January.\n",
      "Money\n",
      "workplace.\n",
      "ya!\n",
      "unknowns.\n",
      "HR?\n"
     ]
    }
   ],
   "source": [
    "job_crisis = torch.torch.topk(embeddings[:,4], NUM).indices\n",
    "for i in range(NUM):\n",
    "    print(corpora.index_word[job_crisis[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
