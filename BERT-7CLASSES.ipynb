{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23682"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('ESConv.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "processed_data = []\n",
    "for i in data:\n",
    "    pt = i['problem_type']\n",
    "    for j in i['dialog']:\n",
    "        processed_data.append([pt, j['content'], j['speaker'], j['annotation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ongoing depression',\n",
       " 'Sometimes we get in a funk, when we do, we can pull ourselves out, I do suggest you write a list of things to do and put on the frige.',\n",
       " 'supporter',\n",
       " {'strategy': 'Others'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[4005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = []\n",
    "for i in processed_data:\n",
    "    sl.append(len(i[1].split()))\n",
    "## Safely, we remove length 1,2,3\n",
    "to_remove = []\n",
    "for i in range(len(sl)):\n",
    "    if sl[i] <= 3:\n",
    "        to_remove.append(i)\n",
    "for index in sorted(to_remove, reverse=True):\n",
    "    del processed_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'job crisis': 7475,\n",
       "         'problems with friends': 4702,\n",
       "         'ongoing depression': 9462,\n",
       "         'breakup with partner': 6697,\n",
       "         'academic pressure': 4009,\n",
       "         'conflict with parents': 255,\n",
       "         'Procrastination': 352,\n",
       "         'Alcohol Abuse': 360,\n",
       "         'Issues with Parents': 239,\n",
       "         'Sleep Problems': 765,\n",
       "         'Appearance Anxiety': 346,\n",
       "         'School Bullying': 46,\n",
       "         'Issues with Children': 371})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cpd = []\n",
    "for i in processed_data:\n",
    "    cpd.append(i[0])\n",
    "Counter(cpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE Issue with parents\n",
    "for i in range(len(processed_data)):\n",
    "    if processed_data[i][0] == 'conflict with parents':\n",
    "        processed_data[i][0] = 'Issues with Parents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE Into Family Issue\n",
    "for i in range(len(processed_data)):\n",
    "    if processed_data[i][0] == 'Issues with Parents':\n",
    "        processed_data[i][0] = 'Family Issues'\n",
    "    if processed_data[i][0] == 'Issues with Children':\n",
    "        processed_data[i][0] = 'Family Issues'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'job crisis': 7475,\n",
       "         'problems with friends': 4702,\n",
       "         'ongoing depression': 9462,\n",
       "         'breakup with partner': 6697,\n",
       "         'academic pressure': 4009,\n",
       "         'Family Issues': 865,\n",
       "         'Procrastination': 352,\n",
       "         'Alcohol Abuse': 360,\n",
       "         'Sleep Problems': 765,\n",
       "         'Appearance Anxiety': 346,\n",
       "         'School Bullying': 46})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpd = []\n",
    "for i in processed_data:\n",
    "    cpd.append(i[0])\n",
    "Counter(cpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we care the all 7 categories.\n",
    "kept = ['problems with friends', 'ongoing depression','breakup with partner', 'academic pressure', 'job crisis', 'Family Issues','Sleep Problems']\n",
    "\n",
    "filtered_processed_data = []\n",
    "for i in processed_data:\n",
    "    if i[0] in kept:\n",
    "        filtered_processed_data.append(list(i))\n",
    "        filtered_processed_data[-1][0] = kept.index(filtered_processed_data[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "random.shuffle(filtered_processed_data)\n",
    "lenth = len(filtered_processed_data)\n",
    "train_list = filtered_processed_data[0:int(lenth*0.8)]\n",
    "test_list = filtered_processed_data[int(lenth*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 5364, 1: 7583, 6: 628, 4: 5986, 3: 3144, 0: 3767, 5: 708})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count labels in training\n",
    "lt = []\n",
    "for i in train_list:\n",
    "    lt.append(i[0])\n",
    "Counter(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 1333, 1: 1879, 3: 865, 4: 1489, 0: 935, 5: 157, 6: 137})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count labels in testing\n",
    "lts = []\n",
    "for i in test_list:\n",
    "    lts.append(i[0])\n",
    "Counter(lts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This would be a classification into 7 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import BatchSampler, Sampler, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, BertModel, AdamW, BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Corpora():\n",
    "    \"\"\"\n",
    "        A class that holds the texts and class labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        \"\"\"\n",
    "            file_path: on MAGIC, it is either\n",
    "                        data-badassnlp/cola_public/raw/in_domain_train.tsv or\n",
    "                        data-badassnlp/cola_public/raw/in_domain_dev.tsv\n",
    "                    on Google Colab, change them to the corresponding paths.\n",
    "        \"\"\"\n",
    "\n",
    "        self.sentences = [s[1] for s in tqdm(l)]\n",
    "        self.labels = [int(s[0]) for s in tqdm(l)]\n",
    "        \n",
    "        \n",
    "class TextClassificationDataSet(Dataset):\n",
    "    \"\"\"\n",
    "        Define a dataset consisting of pairs of (sequence_of_word_indices, class_label).\n",
    "        class_label is either 0 or 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpora, tokenizer: BertTokenizer):\n",
    "        \"\"\"\n",
    "            This function will tokenize the sentences in self.corpora,\n",
    "            and pad the word indices properly.\n",
    "            The tokenization and padding should be done using BERT's API.\n",
    "\n",
    "                See https://huggingface.co/docs/transformers/internal/tokenization_utils\n",
    "                    for the API of tokenizer\n",
    "                See https://huggingface.co/docs/transformers/preprocessing\n",
    "                    for examples of using the API\n",
    "                Also see the BERT paper\n",
    "                    \"BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding\"\n",
    "                regarding how paddings are done.\n",
    "\n",
    "            We turn the entire corpora into a big tensor in a batch, rather than\n",
    "                doing that in the __getitem__ function, as batch processing can speed up tokenization\n",
    "        \n",
    "            Then self.input_ids, self.attention_mask and self.labels\n",
    "            will be created as tensors of shape\n",
    "                torch.Size([num_sentences, max_padded_sentence_length])\n",
    "                torch.Size([num_sentences, max_padded_sentence_length])\n",
    "                torch.Size([num_sentences])\n",
    "\n",
    "            corpora: an object of the Corpora class representing some raw classified texts.\n",
    "            tokenizer: must be the BERTTokenizer loaded from the BERT/download folder\n",
    "                        in order to have the same ids for the words in any vocabulary.\n",
    "                        See the loading codes below.\n",
    "        \"\"\"\n",
    "        self.corpora = corpora\n",
    "        self.tokenizer = tokenizer\n",
    "        d =  tokenizer.batch_encode_plus(corpora.sentences)\n",
    "        padded_encoded_inputs = tokenizer.pad(d)\n",
    "        self.input_ids = torch.tensor(padded_encoded_inputs.input_ids)\n",
    "        self.attention_mask = torch.tensor(padded_encoded_inputs.attention_mask)\n",
    "        self.labels = torch.tensor(corpora.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.corpora.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            Return the idx-th of the rows the self.input_ids, self.attention_mask, self.labels in this order.\n",
    "            Don't do BERT tokenization here as that will be slow.\n",
    "        \"\"\"\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, BERT_model, hidden_layer_size, num_classes):\n",
    "        \n",
    "        super(BERTClassifier, self).__init__()\n",
    "\n",
    "        # loaded pretrained model\n",
    "        self.bert = BERT_model\n",
    "        \n",
    "        # simple neural network that convert embedding of the first token to a class\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(BERT_hidden_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        The following two arguments are tensors from a mini-batch of the input_ids\n",
    "            and attention_mask returned by the BERT tokenizer.\n",
    "            \n",
    "            input_ids: a tensor of shape [batch_size, max_length]\n",
    "            attention_mask: a tensor of shape [batch_size, max_length]\n",
    "            \n",
    "            return: the logits of the sentences in the batch tensor of shape [batch_size, 1, 2]\n",
    "        \"\"\"\n",
    "        # see https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "        # and https://huggingface.co/docs/transformers/main_classes/output\n",
    "        # for the return value of the forward function of BERT\n",
    "\n",
    "        ### Your codes go here (30 points) ###\n",
    "        z = self.bert.forward(input_ids, attention_mask)\n",
    "        return torch.softmax(self.classifier(z[1]).unsqueeze(1), dim=-1) \n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you check the availability of GPU when you set the device ID.\n",
    "device = torch.device(0)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "num_classes = 7\n",
    "classifier_hidden_size = 128\n",
    "\n",
    "## if using MAGIC\n",
    "BERT_PATH = '/data/badassnlp/bert-base-uncased/'\n",
    "## if using Google Colab, you need to load the bert model after it downloads the model files.\n",
    "## \n",
    "\n",
    "BERT_hidden_size = 768\n",
    "\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/badassnlp/bert-base-uncased/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training corpora ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27180/27180 [00:00<00:00, 1865660.47it/s]\n",
      "100%|██████████| 27180/27180 [00:00<00:00, 2482333.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training iterator ... \n",
      "creating test corpora ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6795/6795 [00:00<00:00, 1801649.64it/s]\n",
      "100%|██████████| 6795/6795 [00:00<00:00, 2253344.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating test dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating test iterator ... \n"
     ]
    }
   ],
   "source": [
    "## 'uncased' means all words are lowered-cased before tokenization\n",
    "## 'base' means the smaller version of BERT (12 layers, 16 heads)\n",
    "## un-comment one of the following two options.\n",
    "\n",
    "# if using MAGIC, load from local BERT folder\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH, local_files_only=True)\n",
    "BERT_model = BertModel.from_pretrained(BERT_PATH, num_labels = 2, output_attentions = False, output_hidden_states = False\n",
    ").to(device)\n",
    "\n",
    "## if using Colab, load from automatically downloaded files. Downloading can take half a minute\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# BERT_model = BertModel.from_pretrained(\"bert-base-cased\", num_labels = 2, output_attentions = False, output_hidden_states = False).to(device)\n",
    "\n",
    "\n",
    "## if using MAGIC\n",
    "print(\"creating training corpora ... \")\n",
    "training_corpora = Corpora(train_list)\n",
    "print(\"creating training dataset ... \")\n",
    "training_dataset = TextClassificationDataSet(training_corpora, tokenizer)\n",
    "print(\"creating training iterator ... \")\n",
    "training_iterator = DataLoader(training_dataset, sampler = RandomSampler(training_dataset), batch_size=BATCH_SIZE)\n",
    "\n",
    "## if using MAGIC\n",
    "print(\"creating test corpora ... \")\n",
    "dev_corpora = Corpora(test_list)\n",
    "print(\"creating test dataset ... \")\n",
    "dev_dataset = TextClassificationDataSet(dev_corpora, tokenizer)\n",
    "print(\"creating test iterator ... \")\n",
    "dev_iterator = DataLoader(dev_dataset, sampler = SequentialSampler(dev_dataset), batch_size=BATCH_SIZE)\n",
    "\n",
    "classifier = BERTClassifier(BERT_model, classifier_hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 206])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(training_iterator)[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 163])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dev_iterator)[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam(classifier.parameters())\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "\n",
    "# Global variable that count the number of training epoch\n",
    "num_epochs_train = 0\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"\n",
    "        model: an BERTClassifier object\n",
    "        iterator: a DataLoader object\n",
    "        optimizer: torch optimizer\n",
    "        criterion: the crossentropyloss\n",
    "        clip: to be used with torch.nn.utils.clip_grad_norm_\n",
    "\n",
    "        return: average loss over the training instances (sentences) in the DataLoader.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    num_batchs = 0\n",
    "    total_instances = 0\n",
    "    global num_epochs_train\n",
    "    num_epochs_train += 1\n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        num_batchs += 1\n",
    "        optimizer.zero_grad() \n",
    "        tmp = optimizer.state_dict()\n",
    "        tmp[\"param_groups\"][0][\"lr\"] = 0.00002/(num_epochs_train)\n",
    "        optimizer.load_state_dict(tmp)\n",
    "\n",
    "        ### Your codes go here (10 points) ###\n",
    "\n",
    "        # Step 1 (5 points): get the tensors from this mini-batch and increase the total_instances\n",
    "        # make sure tensors are moved to GPU.\n",
    "        ids = batch[0].to(device)\n",
    "        msk = batch[1].to(device)\n",
    "        y = (batch[2]).to(device)\n",
    "        \n",
    "        # Step 2 (5 points): call the forward function of the model and find the output logits\n",
    "        # then calculate the loss. Then cumulate the epoch_loss\n",
    "        logits = model.forward(ids,msk)\n",
    "        loss = criterion(logits.squeeze(1), y) # if squeeze here, why unsqueeze previous\n",
    "        loss.backward()\n",
    "\n",
    "        # Clips gradient norm of an iterable of parameters.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        total_instances += BATCH_SIZE\n",
    "\n",
    "    return epoch_loss / total_instances\n",
    "\n",
    "confusion_matrix = []\n",
    "num_epochs = 0\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "        Find the loss (criterion) of the model over instances in this DataLoader (iterator)\n",
    "        Same logic as the above train function, except no gradient calculation\n",
    "        and no update of the parameter. \n",
    "        \n",
    "        return: average loss over the training instances (sentences) in the DataLoader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    total_instances = 0\n",
    "    \n",
    "    confusion_matrix.append(torch.zeros(num_classes,num_classes))\n",
    "    global num_epochs\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        \n",
    "        ### Your codes go here (5 points) ###\n",
    "        ids = batch[0].to(device)\n",
    "        msk = batch[1].to(device)\n",
    "        y = (batch[2]).to(device)\n",
    "        logits = model.forward(ids,msk)\n",
    "        loss = criterion(logits.squeeze(1), y) # if squeeze here, why unsqueeze previous\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        total_instances += BATCH_SIZE\n",
    "        \n",
    "\n",
    "        for i in range(len(y)):\n",
    "            row = y[i]\n",
    "            col = torch.argmax((logits.squeeze(1))[i])\n",
    "            confusion_matrix[num_epochs][row][col] += 1\n",
    "    num_epochs += 1\n",
    "\n",
    "    return epoch_loss / total_instances\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "training_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch start:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3398it [08:25,  6.72it/s]\n",
      "850it [00:26, 32.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 8m 52s\tTrain Loss: 1.753 | Test Loss: 1.703\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):  \n",
    "    print(\"epoch start: \", epoch)  \n",
    "    start_time = time.time()\n",
    "    training_loss = train(classifier, training_iterator, optimizer, criterion, CLIP)\n",
    "    training_losses.append(training_loss)\n",
    "    test_loss = evaluate(classifier, dev_iterator, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss \n",
    "        torch.save(classifier.state_dict(), 'best_model_7classes.pt')\n",
    "        \n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s', end='')\n",
    "    print(f'\\tTrain Loss: {training_loss:.3f} | Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([292., 451., 112.,  19.,  61.,   0.,   0.]),\n",
       " tensor([ 112., 1348.,  131.,   50.,  238.,    0.,    0.]),\n",
       " tensor([139., 669., 453.,  15.,  57.,   0.,   0.]),\n",
       " tensor([ 29., 378.,  10., 317., 131.,   0.,   0.]),\n",
       " tensor([ 43., 715.,  46.,  35., 650.,   0.,   0.]),\n",
       " tensor([35., 86., 17.,  3., 16.,  0.,  0.]),\n",
       " tensor([  8., 114.,   3.,   1.,  11.,   0.,   0.])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(confusion_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
